Section 1.2 Procedures and the Processes They Generate
---
* _Procedure_ = A pattern for the _local evolution_ of a computational _process_.
  * This specifies a single stage of a _process_, which may span multiple _procedures_, and describes how each stage is built upon the previous one.
* _Process_ = The course of action specified by a program, and generated by _procedures_.
  * The _global behavior_ of a process is the behavior of a _process_ whose _local evolution_ has been specified by _procedures_.

## 1.2.1 Linear Recursion and Iteration
* _Recursive procedure_ = A _procedure_ whose definition refers to the procedure itself in terms of syntax.
  * Ex1:
    ```scheme
    (define (factorial n)
      (if (= n 1)
          1
          (* n (factorial (- n 1)))))
    ```
  * Ex2: 
    ```scheme
    (define (factorial n)
      (define (iter product counter)
        (if (> counter n)
            product
            (iter (* counter product)
                  (+ counter 1))))
      (iter 1  1))
    ```
* _Recursive process_ = One type of _process_ behavior that a _recursive procedure_ can generate.
  * The recursive reference causes an _expansion_ as the process builds up a chain of _deferred operations_, and then a _contraction_ as the expansions are actually performed.
  * Requires that the interpretor keep track of the operations that still need to be carried out later on.
  * _Linear recursive process_ = When the length of the chain of deferred operations grows _linearly_ with _n_.
    * `Ex1` above is a linear recursive process because it has the following behavior:
      * ` (factorial 6)`
      * ` (* 6 (factorial 5))`
      * ` (* 6 (* 5 (factorial 4)))`
      * ` (* 6 (* 5 (* 4 (factorial 3))))`
      * ` (* 6 (* 5 (* 4 (* 3 (factorial 2)))))`
      * ` (* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))`
      * ` (* 6 (* 5 (* 4 (* 3 (* 2 1)))))`
      * ` (* 6 (* 5 (* 4 (* 3 2))))`
      * ` (* 6 (* 5 (* 4 6)))`
      * ` (* 6 (* 5 24))`
      * ` (* 6 120)`
      * ` 720`
* _Iterative process_ = A process whose state can be summarized by a fixed number of _state variables_, with a fixed rule describing how the state variables should be updated as the process moves from state to state, and an optional end test that specifies conditions for when the process should terminate.
  * Does _not_ grow and shrink.
  * The interpreter only has to keep track of a fixed set of state variables, rather than a list of operations that will need to be computed.
  * The process can be stopped and restarted at any time since the state is held in the variables.
  * For loops & while loops (other languages), and even recursive procedures can all be iterative processes.
    * In the case of recursive procedures, just because the syntax of the definition refers to the procedure itself as a recursive call, the behavior may still be iterative.
  * _Linear iterative process_ = When the number of steps required grows linearly with _n_.
    * `Ex2` above is a linear iterative _process_ (even though it is a recursive _procedure_) because it has the following behavior:
      * ` (factorial 6)`
      * ` (iter   1 1)`
      * ` (iter   1 2)`
      * ` (iter   2 3)`
      * ` (iter   6 4)`
      * ` (iter  24 5)`
      * ` (iter 120 6)`
      * ` (iter 720 7)`
      * ` 720`
    * Notice that the above example does not expand & contract like a recursive process would, even though the syntax makes it a recursive procedure.
    * Also note in `Ex2` that the state variables are `n`, `product`, and `counter`, and that is all that the interpreter needs to keep track of at any given time.
  * _Tail recursive_ = A property of a language implementation in which _iterative processes_ always execute in _constant space_, even if the process is generated by a _recursive procedure_.
    * In other languages, if the procedure definition includes a recursive call, then it always grows the stack size, thus consuming increasing amounts of memory that grows with _n_.
      * So, in those _other_ languages, iterative processes must be defined in the procedure using special _looping constructs_ such as `for`, `while`, `do`, `repeat`, and `until`.
      * In _tail recursive_ language implementations, iterative processes can still be defined cleanly as recursive procedures, such as in `Ex2`.

## 1.2.2 Tree Recursion
* _Tree recursion_ = A recursive process behavior generated by a recursive procedure that contains *2+ recursive calls at each step*.
  * Results in a branching behavior at each step level.
  * The number of steps grows exponentially, and is proportional to the number of nodes in the tree.
  * The space used only grows linearly, since we only keep track of the nodes above at any given point (compute to bottom of branch before continuing to next branch), and is proportional to the maximum depth of the tree.
  * This can result in the _duplication of work_ on different branches of the same tree.
  * Still can be quite useful when designing clean algorithms.
  * Ex: Fibonacci _recursive_ algorithm:
    ```scheme
    (define (fib n)
      (cond ((= n 0) 0)
            ((= n 1) 1)
            (else (+ (fib (- n 1))
                     (fib (- n 2))))))
    ```
  * Ex: compare to the following Fibonacci _iterative_ algorithm:
    ```scheme
    (define (fib n)
      (define (iter a b count)
        (if (= count 0)
            b
            (iter (+ a b) a (- count 1))))
      (iter 1 0 n))
    ```
    * In this case, we are setting `b := a`, and `a := a + b` for `n` steps, then returning `b`.
    * This rewrite of the first Fibonacci algorithm is _iterative_, and thus only has a number of steps that grows _linearly_ with `n`.

## 1.2.3 Orders of Growth
* _Order of growth_ = Used to obtain a gross measure of the resources required by a process as the inputs become larger.
  * Note that the rate of growth of a function is also called its _order_.
  * Let `n` be a parameter that measures the size of the problem, and let `R(n)` be the amount of resources the _process_ requires for a problem of size `n`.
    * `n` could be one of many different properties; best to select a property that describes the size of the problem.
      * Ex: the number for which to compute Fib(n); number of digits to compute the sqrt to; number of rows in a matrix.
    * `R(n)` could be one of many different measurements of resources, and is an **exact** measure of complexity.
      * Ex: number of elementary machine operations; number of internal registers used.
  * **Theta**: `R(n)` has an order of growth `R(n) =  theta(f(n))` (pronounced "theta of f(n)"), if there are positive constants `k_1` and `k_2` independent of `n` such that `k1*f(n) <= R(n) <= k2*f(n)` for a sufficiently large `n`.
    * The idea is that `R(n)` is the **exact** complexity of the process, and `f(n)` is a **bound** on the function `R(n)`.
    * Ex:
      * _Linear recursive process_ = Linear recursive version of the factorial function in 1.2.1.
        * Number of steps grows proportionally to the input `n`, and thus the number of steps required grows at `theta(n)`. The space also grows at `theta(n)`.
        * So, doubling the size `n` will roughly double the amount of resources used.
        * Makes sense because there is a chain of _deferred operations_ that builds up, and then contracts.
        * May be literally more steps than `n`, but the total number is still proportional, as in `k*n`.
      * _Iterative recursive process_ = Iterative recursive version of the factorial function in 1.2.1.
        * Number of steps grows at `theta(n)`, but the space required grows at `theta(1)`, or in other words, is _constant_.
        * Makes sense because only the current state variables have to be remembered, rather than a built up chain of operations.
      * _Tree recursive process_ = Tree-recursive Fibonacci algorithm from 1.2.2.
        * Number of steps grows at `theta(phi^n)`, where `phi` is the golden ratio from 1.2.2, and requires space `theta(n)`.
        * Makes sense because any given branch (computation finishes each branch before moving on to another one) is only `kn` nodes deep, so only a linear amount of space is needed, but the steps grow exponentially.
        * So, doubling the size `n` will multiply the amount of resources used by a constant factor.
      * A process requiring `n^2` steps.
        * `theta (n^2)` growth
      * A process requiring `1000n^2` steps.
        * `theta (n^2)` growth
        * The `1000` is just one of the `k` constants.
      * A process requiring `3n^2 + 10n + 17` steps.
        * `theta (n^2)` growth
        * The fastest growth is the `n^2` term, and the other parts of the problem are miniscule in comparison at large `n`.
  * **Note: _Theta_ is _different_ than _Big-O_ notation** (the following comes from other sources).
    * **Big-O**: `R(n)` has an order of growth with an **upper bound** `R(n) = O(f(n))` if there is a positive constant `k` independent of `n` such that `R(n) <= k*f(n)` for a sufficiently large `n`.
      * So, if `R(n) = O(n^2)`, then we would say that _R(n) grows asymptotically no faster (equal to or slower) than `k*n^2`_.
      * Note that a function that is `O(n^2)` is also `O(n^25)`, `O(n^100)`, or even `O(n^100000)`, since _Big-O_ is just an _upper bound_.
    * **Big-Omega**: `R(n)` has an order of growth with a **lower bound** `R(n) = Omega(f(n))` if there is a positive constant `k` independent of `n` such that `R(n) >= k*f(n)` for a sufficiently large `n`.
    * **Theta (again)**: `R(n)` has an order of growth with a **tight bound** `R(n) = Theta(f(n))` if `R(n) = O(f(n))` and `R(n) = Omega(f(n))`.
      * So, if `R(n) = Theta(f(n))`, then we would say that _R(n) grows at a rate proportional (equal) to `constant*f(n)`_.
      * A function can only be `Theta(f(n))` **iff** it is also `O(f(n))` and `Omega(f(n))`. In other words, the function grows both no slower than and no faster than `constant*f(n)`.
      * Note that if a function is `Theta(n)`, then it can **not** be `Theta(n^2)`, `Theta(n^100)`, or `Theta (1)` because the function has to also be `O(n)` (no greater) and `Omega(n)` (no lower).  Remember that _Theta_ is the tightest bound possible.
    * Others:
      * **Small o**: `R(n)` has an order of growth with an **upper bound** `R(n) = o(f(n))` if there is a positive constant `k` independent of `n` such that `R(n) < k*f(n)` for a sufficiently large `n`.
        * So `R(n)` is lower than but **not equal** to `f(n)`.
      * **Small omega**: `R(n)` has an order of growth with a **lower bound** `R(n) = omega(f(n))` if there is a positive constant `k` independent of `n` such that `R(n) > k*f(n)` for a sufficiently large `n`.
        * So `R(n)` is greater than but **not equal** to `f(n)`.
    * _Theta_ should be used for describing CS algorithms if possible, but is often confused with Big-O.

## 1.2.4 Exponentiation

* Given a base _b_ and a positive integer exponent _n_, `b^n` can be computed as:
  ```scheme
  (define (expt b n)
    (if (= n 0)
        1
        (* b (expt b (- n 1)))))
  ```
  * This is a linear recursive process, with _theta(n)_ steps and _theta(n)_ space.
* Alternatively, we could compute it as:
  ```scheme
  (define (expt b n)
    (define (iter counter product)
      (if (= counter 0)
        product
        (iter (- counter 1) (* b product))))
    (iter n 1))
  ```
  * This is a linear iterative process, with _theta(n)_ steps and _theta(1)_ space.
* We could also reformulate the problem as the exponent of `b^n = b^(n/2) * b^(n/2)`, which would mean that we could reduce the number of steps significantly.
  ```scheme
  (define (fast-expt b n)
    (cond ((= n 0) 1)
          ((even? n) (square (fast-expt b (/ n 2))))
          (else (* b (fast-expt b (- n 1))))))

  (define (even? n)
    (= (remainder n 2) 0))

  (define (square n)
    (* n n))
  ```
  * This is a _logarithmic_ process in both space and steps, and thus is _theta(log n)_ for both.  Note that `b^2n` only takes one more multiplication step than `b^n`, thus this procedure will scale easier.

## 1.2.5 Greatest Common Divisors
* Greatest common divisor (GCD) of two integers _a_ and _b_ is defined as the largest integer that can be divided by both _a_ and _b_ without a remainder. 
* There is a clever algorithm that is based on the observation that if _r_ is the remainder of `a/b`, then the common divisor of _a_ and _b_ is the same as the common divisor of _b_ and _r_.  We then can continue to reduce until _r_ is 0, at which point the GCD of the original _a_ and _b_ will be the current _b_.  This is known as Euclid's Algorithm.
  ```scheme
  (define (gcd a b)
    (if (= b 0)
        a
        (gcd b (remainder a b))))
  ```
  * This generates an iterative process that grows logarithmically in the number of steps.
* **Lame's Theorem**: If Euclid's Algorithm requires _k_ steps to compute the GCD of some pair, then the smaller number of the pair must be greater than or equal to the _k_th Fibonacci number.
  * We can use this to get the order of growth estimate for Euclid's Algorithm.
    * Let _n_ be the smaller of the two inputs.  If the process takes _k_ steps, then we have `n >= Fib(k) ~= phi^k / sqrt (5)`.  This means that the number of steps _k_ grows as the logarithm to the base _phi_ of _n_.  Therefore, the order of growth is _theta(log n)_

## 1.2.6 Example: Testing for Primality
* One way to test if a number is prime is to find the number's divisors.  The number is prime iff the number is its own smallest divisor.
  ```scheme
  (define (prime? n)
    (= n (smallest-divisor n)))

  (define (smallest-divisor n)
    (find-divisor n 2))

  (define (find-divisor n test-divisor)
    (cond ((> (square test-divisor) n) n)
          ((divides? test-divisor n) test-divisor)
          (else (find-divisor n (+ test-divisor 1)))))

  (define (divides? a b)
    (= (remainder b a) 0))
  ```
  * In this algorithm, if _n_ is _not_ prime, then its smallest divisor must be <= sqrt(n).  Thus, the order of growth of the number of steps is _theta(sqrt(n))_.
* There is a _theta(log n)_ primality test based on Fermat's Little Theorem from number theory:
  * **Fermat's Little Theorem**: If _n_ is a prime number and _a_ is any positive integer less than _n_, then _a_ raised to the *n*th power is congruent to _a modulo n_.
    * Two numbers are _congruent modulo n_ if they both have the same remainder when divided by _n_.  
  * So we can build an algorithm from this: Given a number _n_, pick a random number _1 <= a < n_ and compute the remainder of _a^n modulo n_.  If the result is not equal to _a_, then _n_ is not prime.  If it is equal to _a_, then chances are good that _n_ is prime.  Then we pick another random number _a_ and repeat the process.  If this also has the same result, then we are even more confident that _n_ is prime.  Repeat to increase confidence.
  * The following implements a theta(log n) primality tester using Fermat's test:
    ```scheme
    (define (fast-prime? n times)
      (cond ((= times 0) true)
            ((fermat-test n) (fast-prime? n (- times 1)))
            (else false)))

    (define (fermat-test n)
      (define (try-it a)
        (= (expmod a n n) a))
      (try-it (+ 1 (random (- n 1)))))

    (define (expmod base exp m)
      (cond ((= exp 0) 1)
            ((even? exp)
             (remainder (square (expmod base (/ exp 2) m)) m))
            (else (remainder (* base (expmod base (- exp 1) m)) m))))
    ``` 
  * Note that this algorithm is different than most because it is _not_ guaranteed to be correct.
    * If _n_ ever fails the Fermat test, then we are certain that _n_ is _not_ prime.  However, if _n_ passes the test, while this is a very strong indication, it does not guarantee that _n_ is prime.
    * However, the more the algorithm is run, the smaller the chance of error becomes such that we can prove that the chance or error can be pushed to be arbitrarily small.
    * These types of algorithms are known as _probabilistic algorithms_.

